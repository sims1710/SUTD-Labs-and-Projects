{"cells":[{"cell_type":"markdown","metadata":{"id":"oJzKI3DMiYG7"},"source":["# PyTorch Tutorial - Sequence Models\n","\n","In view of the AI quiz, capstone and other deliverables during week 5, the exercises for this lab is made relatively straightforward. The homework exercises can be found at the end of this tutorial."]},{"cell_type":"markdown","source":["## Sentiment detection with BERT\n","\n","The task tackled here is sentiment detection of IMDB articles. In this notebook, we will utilize the Huggingface suite of libraries (transformers, datasets, evaluate, etc) to build upon it to perform some quick prototyping and evaluation of models. Some of the concepts we will use:\n","- loading pre-trained models\n","- fine-tuning on downstream tasks\n","- streamlining the training loop\n","- evaluation"],"metadata":{"id":"Fp-0ZJqT8ehJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q8vt_mfgiYG8"},"outputs":[],"source":["import torch\n","torch.cuda.is_available()"]},{"cell_type":"markdown","source":["First, we install and import various libraries that are useful for this task, namely:\n","- datasets: Contains numerous popular datasets of different modalities and sizes, along with useful pre-processing tools (https://github.com/huggingface/datasets)\n","- transformers: Contains numerous pre-trained model from the huggingface community, along with many useful pipelines for automating the training and evaluation process (https://github.com/huggingface/transformers)\n","- evaluate: Contains numerous standard and custom evaluation metrics that can be easily used as part of the evaluation pipeline (https://github.com/huggingface/evaluate)"],"metadata":{"id":"y-IdwM3Q9_HJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jHMSZlNPiYG_"},"outputs":[],"source":["!pip install -U evaluate\n","!pip install -U datasets\n","!pip install -U accelerate\n","!pip install -U transformers\n","\n","import numpy as np\n","import pandas as pd\n","import evaluate\n","import accelerate\n","from datasets import load_dataset\n","from transformers import AutoTokenizer, pipeline\n","from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n","\n"]},{"cell_type":"markdown","source":["The evaluate library provides many standard evaluation metrics (such as f1, precision, accuracy, etc) for specific tasks. For example, the list of standard metrics for the text classification task can be found at https://huggingface.co/tasks/text-classification. Similarly, there are also many custom-made metrics for specific tasks, which the foillowing code prints out."],"metadata":{"id":"F1m-ETbeG590"}},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"HYHBk5CjiYG_"},"outputs":[],"source":["metrics_list = evaluate.list_evaluation_modules()\n","print(metrics_list)"]},{"cell_type":"markdown","source":["We now load in the dataset of interest for this task, which is the IMDB dataset that contains a series of movie reviews and the corresponding sentiment label."],"metadata":{"id":"RwB8B-1nI0Ye"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"q-SVeAt-iYHA"},"outputs":[],"source":["dataset = load_dataset(\"imdb\")"]},{"cell_type":"code","source":["print(dataset)"],"metadata":{"id":"pt3ohCP4mrBE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Transfer Learning\n","\n","Transfer learning is technique where we take a model that has learnt to solve one problem, then apply it to solving a different but related problem. In this tutorial, we are applying transfer learning by taking the pre-trained BERT-tiny model and fine-tuning it on this task of sentiment detection on IMDB. Details on the BERT-tiny model can be found at https://huggingface.co/google/bert_uncased_L-2_H-128_A-2.\n","\n","The following code loads a model checkpoint from huggingface (i.e., BERT-tiny as defined in the model_checkpoint variable), which you can use to further fine-tune on the specific downstream task (i.e., IMDB sentiment detection). Apart from a standard huggingface model checkpoint, you can also use this to load one of your previously trained model, e.g., to continue more training or transfer learn on another task. This code also performs some pre-processing of the dataset in terms of tokenization and padding/truncating the text to a specific sequence that BERT requires."],"metadata":{"id":"1D0tJ_S6Booo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GuxO_ZyviYHA"},"outputs":[],"source":["model_checkpoint = \"google/bert_uncased_L-2_H-128_A-2\"\n","max_len = 512\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n","\n","def preprocess_function(input_data):\n","    texts = input_data\n","    return tokenizer(texts[\"text\"], max_length=max_len, padding=\"max_length\", truncation=True)\n","\n","encoded_dataset = dataset.map(preprocess_function, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cBixVT5iiYHB"},"outputs":[],"source":["encoded_dataset['train']"]},{"cell_type":"markdown","source":["AutoModelForSequenceClassification provides a high-level interface/wrapper around specific pre-trained models that you can use for general text classification tasks, including finetuning it based on various configurations. More details can be found at https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelforsequenceclassification."],"metadata":{"id":"tofQgqfuKwbZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"KILhS03RiYHB"},"outputs":[],"source":["# model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2, hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1)\n","model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2, hidden_dropout_prob=0.1)\n","model.resize_token_embeddings(len(tokenizer)) # need to resize due to new tokens added"]},{"cell_type":"markdown","source":["Here, TrainingArguments allow us to specific various parameters relating to the training of our model, including training epochs, batch sizes, etc. More details can be found at https://huggingface.co/docs/transformers/v4.15.0/en/main_classes/trainer#transformers.TrainingArguments."],"metadata":{"id":"EICviuNBRpgG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YHbw6fYjiYHB"},"outputs":[],"source":["metric_name = 'f1'\n","model_name = model_checkpoint.split(\"/\")[-1]\n","\n","args = TrainingArguments(\n","    f\"./snapshots/{model_name}-finetuned\",\n","    evaluation_strategy = \"epoch\",\n","    save_strategy = \"epoch\",\n","    save_total_limit = 3,\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    num_train_epochs=2,\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=metric_name,\n","    push_to_hub=False,\n","    fp16=True\n",")"]},{"cell_type":"markdown","source":["For computation of our evaluation metric"],"metadata":{"id":"K3ig_RGPSl7K"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"p9W2ujpyiYHC"},"outputs":[],"source":["metric = evaluate.load(metric_name)\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels, average=\"micro\")"]},{"cell_type":"markdown","source":["Now, we initialize a Trainer that handles the training loop based on our definition of the model and training parameters as defined in the earlier part of this tutorial."],"metadata":{"id":"E8xaX0rRSxwD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UuDfw3v_iYHC"},"outputs":[],"source":["trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=encoded_dataset['train'],\n","    eval_dataset=encoded_dataset['train'],\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")"]},{"cell_type":"markdown","source":["Start the actual model training."],"metadata":{"id":"LxeojkzkTJTm"}},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"1MXcGcMniYHC"},"outputs":[],"source":["train_log = trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yog8tLG8iYHC"},"outputs":[],"source":["# trainer.save_model(\"./models/myFinetunedModel\") # for saving your model"]},{"cell_type":"markdown","source":["Finally, we perform our evaluation on our test set using the fine-tuned model from earlier."],"metadata":{"id":"xgUw9z26TNYW"}},{"cell_type":"code","source":["classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=\"cuda:0\")\n","results = classifier(dataset['test']['text'], max_length=max_len, padding=\"max_length\", truncation=True)\n","dfResults = pd.DataFrame.from_dict(results)\n","dfResults['label'] = dfResults['label'].str.replace('LABEL_','')\n","f1 = metric.compute(predictions=dfResults['label'].tolist(), references=dataset['test']['label'], average='micro')\n","print(f1)"],"metadata":{"id":"xlONu3IXOxZz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Homework Exercises\n","**Due: 1st Mar, 11:59pm**\n","<br>\n","<br>\n","Submit your homework as either: (i) an ipynb file with your results inside; or (ii) a python file and separate pdf discussing your results. Based on what you have done so far in this tutorial, repeat another set of experiments with the following changes:\n","\n","(a) There is a potential error in the design of the training and testing/evaluation step in this tutorial. Identify what it is and describe how you will solve this.\n","\n","(b) Pick another dataset from the datasets library that is not a binary classification task (see https://huggingface.co/datasets). You can sample a subset from this dataset to reduce compute time (see https://huggingface.co/docs/datasets/en/process).\n","\n","(c) Fine-tune the same model but with a dropout of 0.2, over 3 epochs and report on accuracy scores, in addition to F1 scores.\n","\n"],"metadata":{"id":"neCpaLh2TliU"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}