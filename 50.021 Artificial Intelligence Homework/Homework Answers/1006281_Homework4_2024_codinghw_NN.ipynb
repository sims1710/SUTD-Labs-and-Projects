{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NAyKlyGxHT9"
      },
      "source": [
        "# PyTorch Tutorial & Homework - Neural Networks\n",
        "Prof. Lim Kwan Hui, with many thanks to Prof. Dorien Herremans for the initial version and Nelson Lui for the base text.\n",
        "\n",
        "Homework questions are at the end of the tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XCqzzlc3_Zw"
      },
      "source": [
        "# Homework Exercises\n",
        "**Due: 23th Feb, 11:59pm**\n",
        "<br>\n",
        "<br>\n",
        "Based on the same FashionMNIST dataset, work on the following tasks below. Submit your homework as either: (i) an ipynb file with your results inside; or (ii) a python file and separate pdf discussing your results.\n",
        "\n",
        "(a) Develop a new feed-forward neural network that contains 3 hidden layers, with hidden layers 1, 2, 3 being of dimensions 512, 256, 128, respectively. Hidden layer 1 is the layer immediately after the input layer, while hidden layer 3 is the one just before the output layer.\n",
        "\n",
        "(b) Experiment with three different activation functions and two different optimizers. Report your results and discuss your findings.\n",
        "\n",
        "(c) Building upon Task b above, describe and implement two approaches to improve upon the best variation from Task b. Report your results and discuss your findings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpwiTPBeT9ps"
      },
      "source": [
        "# Homework Answers\n",
        "(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4NKVb94FX4qF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import os\n",
        "using_GPU = os.path.exists('/opt/bin/nvidia-smi')\n",
        "\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqzdHx8PQXli",
        "outputId": "4e0ecfc2-752e-48dd-abc3-abcb60168da9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./torchvision-data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 14792536.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./torchvision-data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./torchvision-data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./torchvision-data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 266296.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./torchvision-data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./torchvision-data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./torchvision-data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 4826832.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./torchvision-data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./torchvision-data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./torchvision-data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 9160915.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./torchvision-data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./torchvision-data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torchvision\n",
        "from torchvision.datasets import FashionMNIST\n",
        "\n",
        "train_dataset = FashionMNIST(root='./torchvision-data',\n",
        "                             train=True,\n",
        "                             transform=torchvision.transforms.ToTensor(),\n",
        "                             download=True)\n",
        "\n",
        "test_dataset = FashionMNIST(root='./torchvision-data', train=False,\n",
        "                            transform=torchvision.transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jtzsQ5TOZlqs"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Data-related hyperparameters\n",
        "batch_size = 64\n",
        "\n",
        "# Set up a DataLoader for the training dataset.\n",
        "train_dataloader = DataLoader(\n",
        "    dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Set up a DataLoader for the test dataset.\n",
        "test_dataloader = DataLoader(\n",
        "    dataset=test_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aPKYRlrCpNhq"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNN_new(nn.Module): # ReLU Activation and Dropout\n",
        "    def __init__(self, input_size, num_classes, hidden_dims, dropout, activation_fn):\n",
        "        super(FeedForwardNN_new, self).__init__()\n",
        "\n",
        "        assert len(hidden_dims) == 3\n",
        "\n",
        "        self.hidden_layers = nn.ModuleList([])\n",
        "\n",
        "        self.hidden_layers.append(nn.Linear(input_size, hidden_dims[0]))\n",
        "\n",
        "        for i in range(len(hidden_dims) - 1):\n",
        "          self.hidden_layers.append(nn.Linear(hidden_dims[i], hidden_dims[i + 1]))\n",
        "\n",
        "        # Set up the dropout layer.\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Set up the output layer.\n",
        "        self.output_projection = nn.Linear(128, num_classes)\n",
        "\n",
        "        # Set up the nonlinearity.\n",
        "        self.nonlinearity = activation_fn\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the hidden layers, nonlinearity, and dropout.\n",
        "        for hidden_layer in self.hidden_layers:\n",
        "            x = hidden_layer(x)\n",
        "            x = self.dropout(x)\n",
        "            x = self.nonlinearity(x)\n",
        "\n",
        "        # Output layer: project x to a distribution over classes.\n",
        "        out = self.output_projection(x)\n",
        "\n",
        "        # Softmax the out tensor to get a log-probability distribution\n",
        "        # over classes for each example.\n",
        "        out_distribution = F.log_softmax(out, dim=-1)\n",
        "        return out_distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cB8JFRtIZDz2"
      },
      "source": [
        "Training the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LylCZLFsZCsH"
      },
      "outputs": [],
      "source": [
        "def training_phase(ffnn_optimizer, fashionmnist_ffnn_clf):\n",
        "  # Number of epochs (passes through the dataset) to train the model for.\n",
        "  num_epochs = 10\n",
        "\n",
        "  # A counter for the number of gradient updates we've performed.\n",
        "  num_iter = 0\n",
        "\n",
        "  # Iterate `num_epochs` times.\n",
        "  for epoch in range(num_epochs):\n",
        "    print(\"Starting epoch {}\".format(epoch + 1))\n",
        "    # Iterate over the train_dataloader, unpacking the images and labels\n",
        "    for (images, labels) in train_dataloader:\n",
        "      # Reshape images from (batch_size, 1, 28, 28) to (batch_size, 784), since\n",
        "      # that's what our model expects. Remember that -1 does shape inference!\n",
        "      reshaped_images = images.view(-1, 784)\n",
        "\n",
        "      # Wrap reshaped_images and labels in Variables,\n",
        "      # since we want to calculate gradients and backprop.\n",
        "      reshaped_images = Variable(reshaped_images)\n",
        "      labels = Variable(labels)\n",
        "\n",
        "      # If we're using the GPU, move reshaped_images and labels to the GPU.\n",
        "      if using_GPU:\n",
        "        reshaped_images = reshaped_images.cuda()\n",
        "        labels = labels.cuda()\n",
        "\n",
        "      # Run the forward pass through the model to get predicted log distribution.\n",
        "      # predicted shape: (batch_size, 10) (since there are 10 classes)\n",
        "      predicted = fashionmnist_ffnn_clf(reshaped_images)\n",
        "\n",
        "      # Calculate the loss\n",
        "      batch_loss = nll_criterion(predicted, labels)\n",
        "\n",
        "      # Clear the gradients as we prepare to backprop.\n",
        "      ffnn_optimizer.zero_grad()\n",
        "\n",
        "      # Backprop (backward pass), which calculates gradients.\n",
        "      batch_loss.backward()\n",
        "\n",
        "      # Take a gradient step to update parameters.\n",
        "      ffnn_optimizer.step()\n",
        "\n",
        "      # Increment gradient update counter.\n",
        "      num_iter += 1\n",
        "\n",
        "      # Calculate test set loss and accuracy every 500 gradient updates\n",
        "      # It's standard to have this as a separate evaluate function, but\n",
        "      # we'll place it inline for didactic purposes.\n",
        "      if num_iter % 500 == 0:\n",
        "        # Set model to eval mode, which turns off dropout.\n",
        "        fashionmnist_ffnn_clf.eval()\n",
        "        # Counters for the num of examples we get right / total num of examples.\n",
        "        num_correct = 0\n",
        "        total_examples = 0\n",
        "        total_test_loss = 0\n",
        "\n",
        "        # Iterate over the test dataloader\n",
        "        for (test_images, test_labels) in test_dataloader:\n",
        "          # Reshape images from (batch_size, 1, 28, 28) to (batch_size, 784) again\n",
        "          reshaped_test_images = test_images.view(-1, 784)\n",
        "\n",
        "          # Wrap test data in Variable, like we did earlier.\n",
        "          # We set volatile=True bc we don't need history; speeds up inference.\n",
        "          reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
        "          test_labels = Variable(test_labels, volatile=True)\n",
        "\n",
        "          # If we're using the GPU, move tensors to the GPU.\n",
        "          if using_GPU:\n",
        "            reshaped_test_images = reshaped_test_images.cuda()\n",
        "            test_labels = test_labels.cuda()\n",
        "\n",
        "          # Run the forward pass to get predicted distribution.\n",
        "          predicted = fashionmnist_ffnn_clf(reshaped_test_images)\n",
        "\n",
        "          # Calculate loss for this test batch. This is averaged, so multiply\n",
        "          # by the number of examples in batch to get a total.\n",
        "          total_test_loss += nll_criterion(\n",
        "              predicted, test_labels).data * test_labels.size(0)\n",
        "\n",
        "          # Get predicted labels (argmax)\n",
        "          # We need predicted.data since predicted is a Variable, and torch.max\n",
        "          # expects a Tensor as input. .data extracts Tensor underlying Variable.\n",
        "          _, predicted_labels = torch.max(predicted.data, 1)\n",
        "\n",
        "          # Count the number of examples in this batch\n",
        "          total_examples += test_labels.size(0)\n",
        "\n",
        "          # Count the total number of correctly predicted labels.\n",
        "          # predicted == labels generates a ByteTensor in indices where\n",
        "          # predicted and labels match, so we can sum to get the num correct.\n",
        "          num_correct += torch.sum(predicted_labels == test_labels.data)\n",
        "        accuracy = 100 * num_correct / total_examples\n",
        "        average_test_loss = total_test_loss / total_examples\n",
        "        print(\"Iteration {}. Test Loss {}. Test Accuracy {}.\".format(\n",
        "            num_iter, average_test_loss, accuracy))\n",
        "        # Set the model back to train mode, which activates dropout again.\n",
        "        fashionmnist_ffnn_clf.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h46JhYx7ZU8S",
        "outputId": "8e4c5096-acb8-4ddf-8e60-fdf4b89f8da8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FeedForwardNN_new(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (output_projection): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (nonlinearity): ReLU()\n",
            ")\n",
            "Shapes of model parameters:\n",
            "[torch.Size([512, 784]), torch.Size([512]), torch.Size([256, 512]), torch.Size([256]), torch.Size([128, 256]), torch.Size([128]), torch.Size([10, 128]), torch.Size([10])]\n",
            "Starting epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-e817bedb8a26>:64: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
            "<ipython-input-12-e817bedb8a26>:65: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  test_labels = Variable(test_labels, volatile=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 500. Test Loss 0.5840354561805725. Test Accuracy 76.43999481201172.\n",
            "Starting epoch 2\n",
            "Iteration 1000. Test Loss 0.5898212790489197. Test Accuracy 76.73999786376953.\n",
            "Iteration 1500. Test Loss 0.561018168926239. Test Accuracy 79.5.\n",
            "Starting epoch 3\n",
            "Iteration 2000. Test Loss 0.5525748133659363. Test Accuracy 81.90999603271484.\n",
            "Iteration 2500. Test Loss 0.49628332257270813. Test Accuracy 82.56999969482422.\n",
            "Starting epoch 4\n",
            "Iteration 3000. Test Loss 0.5364255309104919. Test Accuracy 81.0999984741211.\n",
            "Iteration 3500. Test Loss 0.5090486407279968. Test Accuracy 82.95999908447266.\n",
            "Starting epoch 5\n",
            "Iteration 4000. Test Loss 0.5092098712921143. Test Accuracy 82.3699951171875.\n",
            "Iteration 4500. Test Loss 0.46504807472229004. Test Accuracy 84.22000122070312.\n",
            "Starting epoch 6\n",
            "Iteration 5000. Test Loss 0.4656705856323242. Test Accuracy 83.77999877929688.\n",
            "Iteration 5500. Test Loss 0.477907657623291. Test Accuracy 83.20999908447266.\n",
            "Starting epoch 7\n",
            "Iteration 6000. Test Loss 0.5084761381149292. Test Accuracy 79.56999969482422.\n",
            "Iteration 6500. Test Loss 0.46877458691596985. Test Accuracy 83.47999572753906.\n",
            "Starting epoch 8\n",
            "Iteration 7000. Test Loss 0.45668846368789673. Test Accuracy 83.62999725341797.\n",
            "Iteration 7500. Test Loss 0.4650321304798126. Test Accuracy 84.54999542236328.\n",
            "Starting epoch 9\n",
            "Iteration 8000. Test Loss 0.44764548540115356. Test Accuracy 84.33999633789062.\n",
            "Starting epoch 10\n",
            "Iteration 8500. Test Loss 0.49915745854377747. Test Accuracy 82.40999603271484.\n",
            "Iteration 9000. Test Loss 0.4564763009548187. Test Accuracy 84.0199966430664.\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the new feed-forward neural network.\n",
        "relu_ffnn_clf = FeedForwardNN_new(input_size=784, num_classes=10, hidden_dims = [512, 256, 128], dropout=0.2, activation_fn= nn.ReLU())\n",
        "if using_GPU:\n",
        "    relu_ffnn_clf = relu_ffnn_clf.cuda()\n",
        "\n",
        "print(relu_ffnn_clf)\n",
        "\n",
        "parameters = relu_ffnn_clf.parameters()\n",
        "\n",
        "print(\"Shapes of model parameters:\")\n",
        "print([x.size() for x in list(parameters)])\n",
        "\n",
        "# Set up criterion for calculating loss\n",
        "nll_criterion = nn.NLLLoss()\n",
        "\n",
        "lr = 0.1\n",
        "momentum = 0.9\n",
        "# Set up an optimizer for updating the parameters of fashionmnist_ffnn_clf\n",
        "SGD_optimizer = optim.SGD(relu_ffnn_clf.parameters(),\n",
        "                           lr=lr, momentum=momentum)\n",
        "\n",
        "ffnn_optimizer = SGD_optimizer\n",
        "training_phase(ffnn_optimizer, relu_ffnn_clf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9bLiDZRYTq5"
      },
      "source": [
        "(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMTLSRuqY0kD",
        "outputId": "b4c6973e-591a-451c-b03f-dfeaf2152aeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FeedForwardNN_new(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (output_projection): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (nonlinearity): Tanh()\n",
            ")\n",
            "Shapes of model parameters:\n",
            "[torch.Size([512, 784]), torch.Size([512]), torch.Size([256, 512]), torch.Size([256]), torch.Size([128, 256]), torch.Size([128]), torch.Size([10, 128]), torch.Size([10])]\n",
            "Starting epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-e817bedb8a26>:64: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
            "<ipython-input-12-e817bedb8a26>:65: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  test_labels = Variable(test_labels, volatile=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 500. Test Loss 0.6184403300285339. Test Accuracy 80.4000015258789.\n",
            "Starting epoch 2\n",
            "Iteration 1000. Test Loss 0.7050087451934814. Test Accuracy 74.7699966430664.\n",
            "Iteration 1500. Test Loss 0.5966458916664124. Test Accuracy 82.13999938964844.\n",
            "Starting epoch 3\n",
            "Iteration 2000. Test Loss 0.7283454537391663. Test Accuracy 77.72999572753906.\n",
            "Iteration 2500. Test Loss 0.6101191639900208. Test Accuracy 80.62999725341797.\n",
            "Starting epoch 4\n",
            "Iteration 3000. Test Loss 0.5985407829284668. Test Accuracy 81.93000030517578.\n",
            "Iteration 3500. Test Loss 0.6728845834732056. Test Accuracy 81.0999984741211.\n",
            "Starting epoch 5\n",
            "Iteration 4000. Test Loss 0.5841867327690125. Test Accuracy 81.79000091552734.\n",
            "Iteration 4500. Test Loss 0.5688333511352539. Test Accuracy 82.41999816894531.\n",
            "Starting epoch 6\n",
            "Iteration 5000. Test Loss 0.6561270356178284. Test Accuracy 81.08999633789062.\n",
            "Iteration 5500. Test Loss 0.5494189262390137. Test Accuracy 83.37999725341797.\n",
            "Starting epoch 7\n",
            "Iteration 6000. Test Loss 0.658323347568512. Test Accuracy 81.11000061035156.\n",
            "Iteration 6500. Test Loss 0.5320541262626648. Test Accuracy 83.18999481201172.\n",
            "Starting epoch 8\n",
            "Iteration 7000. Test Loss 0.6139629483222961. Test Accuracy 82.73999786376953.\n",
            "Iteration 7500. Test Loss 0.5557726621627808. Test Accuracy 83.47999572753906.\n",
            "Starting epoch 9\n",
            "Iteration 8000. Test Loss 0.5952974557876587. Test Accuracy 82.04000091552734.\n",
            "Starting epoch 10\n",
            "Iteration 8500. Test Loss 0.6839349865913391. Test Accuracy 81.19999694824219.\n",
            "Iteration 9000. Test Loss 0.5417569875717163. Test Accuracy 83.5199966430664.\n"
          ]
        }
      ],
      "source": [
        "# Tanh Activation Function + SGD Optimizer\n",
        "# Instantiate the new feed-forward neural network.\n",
        "tanh_ffnn_clf = FeedForwardNN_new(input_size=784, num_classes=10, hidden_dims = [512, 256, 128], dropout=0.2, activation_fn= nn.Tanh())\n",
        "if using_GPU:\n",
        "    tanh_ffnn_clf = tanh_ffnn_clf.cuda()\n",
        "\n",
        "print(tanh_ffnn_clf)\n",
        "\n",
        "parameters = tanh_ffnn_clf.parameters()\n",
        "\n",
        "print(\"Shapes of model parameters:\")\n",
        "print([x.size() for x in list(parameters)])\n",
        "\n",
        "# Set up criterion for calculating loss\n",
        "nll_criterion = nn.NLLLoss()\n",
        "\n",
        "lr = 0.1\n",
        "momentum = 0.9\n",
        "# Set up an optimizer for updating the parameters of fashionmnist_ffnn_clf\n",
        "SGD_optimizer = optim.SGD(tanh_ffnn_clf.parameters(),\n",
        "                           lr=lr, momentum=momentum)\n",
        "\n",
        "ffnn_optimizer = SGD_optimizer\n",
        "training_phase(ffnn_optimizer, tanh_ffnn_clf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wpQ8OCMeFg8",
        "outputId": "5dba4329-6bdb-4694-b539-a3b96f6a2e8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FeedForwardNN_new(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (output_projection): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (nonlinearity): Sigmoid()\n",
            ")\n",
            "Shapes of model parameters:\n",
            "[torch.Size([512, 784]), torch.Size([512]), torch.Size([256, 512]), torch.Size([256]), torch.Size([128, 256]), torch.Size([128]), torch.Size([10, 128]), torch.Size([10])]\n",
            "Starting epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-e817bedb8a26>:64: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
            "<ipython-input-12-e817bedb8a26>:65: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  test_labels = Variable(test_labels, volatile=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 500. Test Loss 2.304335832595825. Test Accuracy 10.0.\n",
            "Starting epoch 2\n",
            "Iteration 1000. Test Loss 2.157402992248535. Test Accuracy 19.53999900817871.\n",
            "Iteration 1500. Test Loss 0.8585001826286316. Test Accuracy 65.12999725341797.\n",
            "Starting epoch 3\n",
            "Iteration 2000. Test Loss 0.7803479433059692. Test Accuracy 72.16999816894531.\n",
            "Iteration 2500. Test Loss 0.6224414110183716. Test Accuracy 78.55999755859375.\n",
            "Starting epoch 4\n",
            "Iteration 3000. Test Loss 0.5974296927452087. Test Accuracy 80.13999938964844.\n",
            "Iteration 3500. Test Loss 0.5586544871330261. Test Accuracy 81.57999420166016.\n",
            "Starting epoch 5\n",
            "Iteration 4000. Test Loss 0.49993616342544556. Test Accuracy 83.12999725341797.\n",
            "Iteration 4500. Test Loss 0.5281434059143066. Test Accuracy 82.8699951171875.\n",
            "Starting epoch 6\n",
            "Iteration 5000. Test Loss 0.5730727910995483. Test Accuracy 82.30999755859375.\n",
            "Iteration 5500. Test Loss 0.48799291253089905. Test Accuracy 83.62999725341797.\n",
            "Starting epoch 7\n",
            "Iteration 6000. Test Loss 0.4766145944595337. Test Accuracy 84.37999725341797.\n",
            "Iteration 6500. Test Loss 0.4533001482486725. Test Accuracy 85.19999694824219.\n",
            "Starting epoch 8\n",
            "Iteration 7000. Test Loss 0.46994534134864807. Test Accuracy 84.31999969482422.\n",
            "Iteration 7500. Test Loss 0.45036160945892334. Test Accuracy 85.07999420166016.\n",
            "Starting epoch 9\n",
            "Iteration 8000. Test Loss 0.46349427103996277. Test Accuracy 84.72000122070312.\n",
            "Starting epoch 10\n",
            "Iteration 8500. Test Loss 0.4327780306339264. Test Accuracy 85.93999481201172.\n",
            "Iteration 9000. Test Loss 0.507725179195404. Test Accuracy 83.44999694824219.\n"
          ]
        }
      ],
      "source": [
        "# Sigmoid Activation Function + SGD Optimizer\n",
        "# Instantiate the new feed-forward neural network.\n",
        "sigmoid_ffnn_clf = FeedForwardNN_new(input_size=784, num_classes=10, hidden_dims = [512, 256, 128], dropout=0.2, activation_fn= nn.Sigmoid())\n",
        "if using_GPU:\n",
        "    sigmoid_ffnn_clf = sigmoid_ffnn_clf.cuda()\n",
        "\n",
        "print(sigmoid_ffnn_clf)\n",
        "\n",
        "parameters = sigmoid_ffnn_clf.parameters()\n",
        "\n",
        "print(\"Shapes of model parameters:\")\n",
        "print([x.size() for x in list(parameters)])\n",
        "\n",
        "# Set up criterion for calculating loss\n",
        "nll_criterion = nn.NLLLoss()\n",
        "\n",
        "lr = 0.1\n",
        "momentum = 0.9\n",
        "# Set up an optimizer for updating the parameters of fashionmnist_ffnn_clf\n",
        "SGD_optimizer = optim.SGD(sigmoid_ffnn_clf.parameters(),\n",
        "                           lr=lr, momentum=momentum)\n",
        "\n",
        "ffnn_optimizer = SGD_optimizer\n",
        "training_phase(ffnn_optimizer, sigmoid_ffnn_clf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyBeyF5keqkB",
        "outputId": "f3f2bcae-dca5-4cae-b5e1-e5dd764d568d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FeedForwardNN_new(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (output_projection): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (nonlinearity): ReLU()\n",
            ")\n",
            "Shapes of model parameters:\n",
            "[torch.Size([512, 784]), torch.Size([512]), torch.Size([256, 512]), torch.Size([256]), torch.Size([128, 256]), torch.Size([128]), torch.Size([10, 128]), torch.Size([10])]\n",
            "Starting epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-b294d5e67d59>:64: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
            "<ipython-input-9-b294d5e67d59>:65: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  test_labels = Variable(test_labels, volatile=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 500. Test Loss 0.6407455801963806. Test Accuracy 77.02999877929688.\n",
            "Starting epoch 2\n",
            "Iteration 1000. Test Loss 0.5363321900367737. Test Accuracy 80.69000244140625.\n",
            "Iteration 1500. Test Loss 0.4833813011646271. Test Accuracy 83.16999816894531.\n",
            "Starting epoch 3\n",
            "Iteration 2000. Test Loss 0.4567480981349945. Test Accuracy 83.61000061035156.\n",
            "Iteration 2500. Test Loss 0.4402889609336853. Test Accuracy 84.25.\n",
            "Starting epoch 4\n",
            "Iteration 3000. Test Loss 0.418686181306839. Test Accuracy 84.70999908447266.\n",
            "Iteration 3500. Test Loss 0.4084586203098297. Test Accuracy 85.22000122070312.\n",
            "Starting epoch 5\n",
            "Iteration 4000. Test Loss 0.39851099252700806. Test Accuracy 85.56999969482422.\n",
            "Iteration 4500. Test Loss 0.3913775384426117. Test Accuracy 85.77999877929688.\n",
            "Starting epoch 6\n",
            "Iteration 5000. Test Loss 0.38298240303993225. Test Accuracy 85.95999908447266.\n",
            "Iteration 5500. Test Loss 0.3737016022205353. Test Accuracy 86.37999725341797.\n",
            "Starting epoch 7\n",
            "Iteration 6000. Test Loss 0.3758830428123474. Test Accuracy 86.62000274658203.\n",
            "Iteration 6500. Test Loss 0.36740806698799133. Test Accuracy 86.68000030517578.\n",
            "Starting epoch 8\n",
            "Iteration 7000. Test Loss 0.3612154424190521. Test Accuracy 87.02999877929688.\n",
            "Iteration 7500. Test Loss 0.35612592101097107. Test Accuracy 87.19000244140625.\n",
            "Starting epoch 9\n",
            "Iteration 8000. Test Loss 0.3561447262763977. Test Accuracy 87.12999725341797.\n",
            "Starting epoch 10\n",
            "Iteration 8500. Test Loss 0.35720667243003845. Test Accuracy 86.95999908447266.\n",
            "Iteration 9000. Test Loss 0.3575669825077057. Test Accuracy 87.06999969482422.\n"
          ]
        }
      ],
      "source": [
        "# ReLU Activation Function + Adam Optimizer\n",
        "# Instantiate the new feed-forward neural network.\n",
        "relu_ffnn_clf_2 = FeedForwardNN_new(input_size=784, num_classes=10, hidden_dims = [512, 256, 128], dropout=0.2, activation_fn= nn.ReLU())\n",
        "if using_GPU:\n",
        "    relu_ffnn_clf_2 = relu_ffnn_clf_2.cuda()\n",
        "\n",
        "print(relu_ffnn_clf_2)\n",
        "\n",
        "parameters = relu_ffnn_clf_2.parameters()\n",
        "\n",
        "print(\"Shapes of model parameters:\")\n",
        "print([x.size() for x in list(parameters)])\n",
        "\n",
        "# Set up criterion for calculating loss\n",
        "nll_criterion = nn.NLLLoss()\n",
        "\n",
        "lr = 0.1\n",
        "momentum = 0.9\n",
        "# Set up an optimizer for updating the parameters of fashionmnist_ffnn_clf\n",
        "Adam_optimizer = optim.Adam(relu_ffnn_clf_2.parameters(), lr=0.0001)\n",
        "\n",
        "ffnn_optimizer = Adam_optimizer\n",
        "training_phase(ffnn_optimizer, relu_ffnn_clf_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rb8EVCccevm_",
        "outputId": "3a3e67a0-7d94-49fa-cb71-51b4c7101bad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FeedForwardNN_new(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (output_projection): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (nonlinearity): Tanh()\n",
            ")\n",
            "Shapes of model parameters:\n",
            "[torch.Size([512, 784]), torch.Size([512]), torch.Size([256, 512]), torch.Size([256]), torch.Size([128, 256]), torch.Size([128]), torch.Size([10, 128]), torch.Size([10])]\n",
            "Starting epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-e817bedb8a26>:64: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
            "<ipython-input-12-e817bedb8a26>:65: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  test_labels = Variable(test_labels, volatile=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 500. Test Loss 0.6105337738990784. Test Accuracy 78.37999725341797.\n",
            "Starting epoch 2\n",
            "Iteration 1000. Test Loss 0.5047974586486816. Test Accuracy 81.87999725341797.\n",
            "Iteration 1500. Test Loss 0.4703048765659332. Test Accuracy 83.16999816894531.\n",
            "Starting epoch 3\n",
            "Iteration 2000. Test Loss 0.45681488513946533. Test Accuracy 83.97000122070312.\n",
            "Iteration 2500. Test Loss 0.447513222694397. Test Accuracy 84.06999969482422.\n",
            "Starting epoch 4\n",
            "Iteration 3000. Test Loss 0.43859827518463135. Test Accuracy 84.68000030517578.\n",
            "Iteration 3500. Test Loss 0.4257950484752655. Test Accuracy 85.18000030517578.\n",
            "Starting epoch 5\n",
            "Iteration 4000. Test Loss 0.42296990752220154. Test Accuracy 85.48999786376953.\n",
            "Iteration 4500. Test Loss 0.41380956768989563. Test Accuracy 85.79999542236328.\n",
            "Starting epoch 6\n",
            "Iteration 5000. Test Loss 0.4294901192188263. Test Accuracy 84.73999786376953.\n",
            "Iteration 5500. Test Loss 0.4085720479488373. Test Accuracy 85.56999969482422.\n",
            "Starting epoch 7\n",
            "Iteration 6000. Test Loss 0.4097915291786194. Test Accuracy 85.75.\n",
            "Iteration 6500. Test Loss 0.4001024663448334. Test Accuracy 86.25999450683594.\n",
            "Starting epoch 8\n",
            "Iteration 7000. Test Loss 0.4009915888309479. Test Accuracy 86.25.\n",
            "Iteration 7500. Test Loss 0.39619773626327515. Test Accuracy 86.11000061035156.\n",
            "Starting epoch 9\n",
            "Iteration 8000. Test Loss 0.39638906717300415. Test Accuracy 86.36000061035156.\n",
            "Starting epoch 10\n",
            "Iteration 8500. Test Loss 0.39068588614463806. Test Accuracy 86.77999877929688.\n",
            "Iteration 9000. Test Loss 0.38600417971611023. Test Accuracy 86.81999969482422.\n"
          ]
        }
      ],
      "source": [
        "# Tanh Activation Function + Adam Optimizer\n",
        "# Instantiate the new feed-forward neural network.\n",
        "tanh_ffnn_clf_2 = FeedForwardNN_new(input_size=784, num_classes=10, hidden_dims = [512, 256, 128], dropout=0.2, activation_fn= nn.Tanh())\n",
        "if using_GPU:\n",
        "    tanh_ffnn_clf_2 = tanh_ffnn_clf_2.cuda()\n",
        "\n",
        "print(tanh_ffnn_clf_2)\n",
        "\n",
        "parameters = tanh_ffnn_clf_2.parameters()\n",
        "\n",
        "print(\"Shapes of model parameters:\")\n",
        "print([x.size() for x in list(parameters)])\n",
        "\n",
        "# Set up criterion for calculating loss\n",
        "nll_criterion = nn.NLLLoss()\n",
        "\n",
        "lr = 0.1\n",
        "momentum = 0.9\n",
        "# Set up an optimizer for updating the parameters of fashionmnist_ffnn_clf\n",
        "Adam_optimizer = optim.Adam(tanh_ffnn_clf_2.parameters(), lr=0.0001)\n",
        "\n",
        "ffnn_optimizer = Adam_optimizer\n",
        "training_phase(ffnn_optimizer, fashionmnist_ffnn_clf = tanh_ffnn_clf_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1amaRpvezhI",
        "outputId": "102a1168-6f75-4c2b-9d31-aa30fb7afad7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FeedForwardNN_new(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (output_projection): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (nonlinearity): Sigmoid()\n",
            ")\n",
            "Shapes of model parameters:\n",
            "[torch.Size([512, 784]), torch.Size([512]), torch.Size([256, 512]), torch.Size([256]), torch.Size([128, 256]), torch.Size([128]), torch.Size([10, 128]), torch.Size([10])]\n",
            "Starting epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-b294d5e67d59>:64: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
            "<ipython-input-9-b294d5e67d59>:65: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  test_labels = Variable(test_labels, volatile=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 500. Test Loss 1.625672459602356. Test Accuracy 33.779998779296875.\n",
            "Starting epoch 2\n",
            "Iteration 1000. Test Loss 1.256834626197815. Test Accuracy 50.63999938964844.\n",
            "Iteration 1500. Test Loss 1.075071096420288. Test Accuracy 56.290000915527344.\n",
            "Starting epoch 3\n",
            "Iteration 2000. Test Loss 0.9339541792869568. Test Accuracy 61.06999969482422.\n",
            "Iteration 2500. Test Loss 0.8313959836959839. Test Accuracy 66.37999725341797.\n",
            "Starting epoch 4\n",
            "Iteration 3000. Test Loss 0.7557201385498047. Test Accuracy 70.83999633789062.\n",
            "Iteration 3500. Test Loss 0.6834837794303894. Test Accuracy 72.2300033569336.\n",
            "Starting epoch 5\n",
            "Iteration 4000. Test Loss 0.6570406556129456. Test Accuracy 73.6500015258789.\n",
            "Iteration 4500. Test Loss 0.6209216117858887. Test Accuracy 75.7300033569336.\n",
            "Starting epoch 6\n",
            "Iteration 5000. Test Loss 0.605666995048523. Test Accuracy 76.08000183105469.\n",
            "Iteration 5500. Test Loss 0.5829781889915466. Test Accuracy 77.2300033569336.\n",
            "Starting epoch 7\n",
            "Iteration 6000. Test Loss 0.5786867737770081. Test Accuracy 77.06999969482422.\n",
            "Iteration 6500. Test Loss 0.5593945980072021. Test Accuracy 78.62999725341797.\n",
            "Starting epoch 8\n",
            "Iteration 7000. Test Loss 0.5662854313850403. Test Accuracy 78.83000183105469.\n",
            "Iteration 7500. Test Loss 0.5597488880157471. Test Accuracy 78.2699966430664.\n",
            "Starting epoch 9\n",
            "Iteration 8000. Test Loss 0.5327855944633484. Test Accuracy 80.12999725341797.\n",
            "Starting epoch 10\n",
            "Iteration 8500. Test Loss 0.5283542275428772. Test Accuracy 80.66999816894531.\n",
            "Iteration 9000. Test Loss 0.5228062868118286. Test Accuracy 80.77999877929688.\n"
          ]
        }
      ],
      "source": [
        "# Sigmoid Activation Function + Adam Optimizer\n",
        "sigmoid_ffnn_clf_2 = FeedForwardNN_new(input_size=784, num_classes=10, hidden_dims = [512, 256, 128], dropout=0.2, activation_fn= nn.Sigmoid())\n",
        "if using_GPU:\n",
        "    sigmoid_ffnn_clf_2 = sigmoid_ffnn_clf_2.cuda()\n",
        "\n",
        "print(sigmoid_ffnn_clf_2)\n",
        "\n",
        "parameters = sigmoid_ffnn_clf_2.parameters()\n",
        "\n",
        "print(\"Shapes of model parameters:\")\n",
        "print([x.size() for x in list(parameters)])\n",
        "\n",
        "# Set up criterion for calculating loss\n",
        "nll_criterion = nn.NLLLoss()\n",
        "\n",
        "lr = 0.1\n",
        "momentum = 0.9\n",
        "# Set up an optimizer for updating the parameters of fashionmnist_ffnn_clf\n",
        "Adam_optimizer = optim.Adam(sigmoid_ffnn_clf_2.parameters(), lr=0.0001)\n",
        "\n",
        "ffnn_optimizer = Adam_optimizer\n",
        "training_phase(ffnn_optimizer, sigmoid_ffnn_clf_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p68vFIOkm_R4"
      },
      "source": [
        "**Report on the activation functions and optimizers:**\n",
        "\n",
        "**Results:**\n",
        "- ReLU consistently outperformed TanH and Sigmoid, exhibiting the lowest test loss and highest test accuracy.\n",
        "- Adam optimizer consistently surpassed SGD, demonstrating lower test loss and higher test accuracy across all activation functions.\n",
        "- The most effective configuration was ReLU activation with Adam optimizer, showcasing superior performance in both test loss and accuracy.\n",
        "\n",
        "**Discussion:**\n",
        "\n",
        "The superior performance of ReLU activation may stem from its ability to mitigate the vanishing gradient problem. Adam's adaptive learning rate likely contributes to its effectiveness in optimizing neural networks.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "Based on the experiments, ReLU with Adam optimizer emerges as the optimal choice for neural network training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxF2bWN6YJNm"
      },
      "source": [
        "(c) To enhance the performance of the top-performing variation, I will implement the following two strategies:\n",
        "\n",
        "1. **L2 Regularization**: I will incorporate an L2 penalty term into the Adam optimizer function to implement L2 regularization, encouraging sparse weights in the model.\n",
        "\n",
        "2. **Early Stopping**: I will monitor the validation loss during training and halt the training process when the validation loss ceases to improve or begins to increase consistently over multiple epochs to implement early stopping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47GMCBHhcXcx"
      },
      "source": [
        "**1. L2 Regularization:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkYsDMGFcxil",
        "outputId": "2b028ab1-adbf-4b26-ab47-e2a51fb1577e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FeedForwardNN_new(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (output_projection): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (nonlinearity): ReLU()\n",
            ")\n",
            "Shapes of model parameters:\n",
            "[torch.Size([512, 784]), torch.Size([512]), torch.Size([256, 512]), torch.Size([256]), torch.Size([128, 256]), torch.Size([128]), torch.Size([10, 128]), torch.Size([10])]\n",
            "Starting epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-b294d5e67d59>:64: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
            "<ipython-input-9-b294d5e67d59>:65: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  test_labels = Variable(test_labels, volatile=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 500. Test Loss 0.6415849328041077. Test Accuracy 76.8499984741211.\n",
            "Starting epoch 2\n",
            "Iteration 1000. Test Loss 0.5313308835029602. Test Accuracy 80.95999908447266.\n",
            "Iteration 1500. Test Loss 0.48429322242736816. Test Accuracy 82.55000305175781.\n",
            "Starting epoch 3\n",
            "Iteration 2000. Test Loss 0.46114739775657654. Test Accuracy 83.72000122070312.\n",
            "Iteration 2500. Test Loss 0.4384508430957794. Test Accuracy 84.29000091552734.\n",
            "Starting epoch 4\n",
            "Iteration 3000. Test Loss 0.42017748951911926. Test Accuracy 85.0999984741211.\n",
            "Iteration 3500. Test Loss 0.4081423580646515. Test Accuracy 85.25.\n",
            "Starting epoch 5\n",
            "Iteration 4000. Test Loss 0.4004044532775879. Test Accuracy 85.76000213623047.\n",
            "Iteration 4500. Test Loss 0.39404451847076416. Test Accuracy 85.8499984741211.\n",
            "Starting epoch 6\n",
            "Iteration 5000. Test Loss 0.3809807002544403. Test Accuracy 86.45999908447266.\n",
            "Iteration 5500. Test Loss 0.3748464286327362. Test Accuracy 86.58999633789062.\n",
            "Starting epoch 7\n",
            "Iteration 6000. Test Loss 0.37384873628616333. Test Accuracy 86.33999633789062.\n",
            "Iteration 6500. Test Loss 0.3899543881416321. Test Accuracy 85.61000061035156.\n",
            "Starting epoch 8\n",
            "Iteration 7000. Test Loss 0.36253538727760315. Test Accuracy 86.94000244140625.\n",
            "Iteration 7500. Test Loss 0.36069056391716003. Test Accuracy 87.04000091552734.\n",
            "Starting epoch 9\n",
            "Iteration 8000. Test Loss 0.350813090801239. Test Accuracy 87.43000030517578.\n",
            "Starting epoch 10\n",
            "Iteration 8500. Test Loss 0.34779539704322815. Test Accuracy 87.41000366210938.\n",
            "Iteration 9000. Test Loss 0.342876672744751. Test Accuracy 87.51000213623047.\n"
          ]
        }
      ],
      "source": [
        "# L2 regularization + Dropout on the best combination: ReLU Activation Function + Adam Optimizer\n",
        "l2_ffnn_clf = FeedForwardNN_new(input_size=784, num_classes=10, hidden_dims = [512, 256, 128], dropout=0.2, activation_fn= nn.ReLU())\n",
        "if using_GPU:\n",
        "    l2_ffnn_clf = l2_ffnn_clf.cuda()\n",
        "\n",
        "print(l2_ffnn_clf)\n",
        "\n",
        "parameters = l2_ffnn_clf.parameters()\n",
        "\n",
        "print(\"Shapes of model parameters:\")\n",
        "print([x.size() for x in list(parameters)])\n",
        "\n",
        "# Set up criterion for calculating loss\n",
        "nll_criterion = nn.NLLLoss()\n",
        "\n",
        "lr = 0.1\n",
        "momentum = 0.9\n",
        "# Set up an optimizer for updating the parameters of fashionmnist_ffnn_clf\n",
        "Adam_optimizer = optim.Adam(l2_ffnn_clf.parameters(), lr=0.0001, weight_decay=1e-5) #L2 penalty: weight_decay\n",
        "\n",
        "ffnn_optimizer = Adam_optimizer\n",
        "training_phase(ffnn_optimizer, l2_ffnn_clf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_eJyPMccj-w"
      },
      "source": [
        "**Report on L2 Regularization:**\n",
        "\n",
        "**Results:**\n",
        "- Addition of the L2 penalty to the Adam Optimizer resulted in a slight improvement in test loss and test accuracy.\n",
        "- The test loss decreased from 0.3576 to 0.3429, while the test accuracy increased from 87.07% to 87.51%.\n",
        "\n",
        "**Discussion:**\n",
        "\n",
        "The observed improvement in performance with the L2 penalty suggests that regularization helped in reducing overfitting and enhancing generalization. This regularization technique likely helped in controlling the complexity of the model and improving its robustness.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "The incorporation of the L2 penalty into the Adam Optimizer led to a modest yet meaningful enhancement in the model's performance, as indicated by the decrease in test loss and increase in test accuracy. This demonstrates the efficacy of regularization techniques in improving the performance of neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INyd001scsWH"
      },
      "source": [
        "**2. Early Stopping:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BJrFd-dWffTU"
      },
      "outputs": [],
      "source": [
        "def early_stop_training_phase(ffnn_optimizer, fashionmnist_ffnn_clf, patience):\n",
        "    # Number of epochs (passes through the dataset) to train the model for.\n",
        "    num_epochs = 10\n",
        "\n",
        "    # A counter for the number of gradient updates we've performed.\n",
        "    num_iter = 0\n",
        "\n",
        "    # Initialize variables for early stopping\n",
        "    best_test_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    # Iterate `num_epochs` times.\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"Starting epoch {}\".format(epoch + 1))\n",
        "        # Iterate over the train_dataloader, unpacking the images and labels\n",
        "        for (images, labels) in train_dataloader:\n",
        "            # Reshape images from (batch_size, 1, 28, 28) to (batch_size, 784), since\n",
        "            # that's what our model expects. Remember that -1 does shape inference!\n",
        "            reshaped_images = images.view(-1, 784)\n",
        "\n",
        "            # Wrap reshaped_images and labels in Variables,\n",
        "            # since we want to calculate gradients and backprop.\n",
        "            reshaped_images = Variable(reshaped_images)\n",
        "            labels = Variable(labels)\n",
        "\n",
        "            # If we're using the GPU, move reshaped_images and labels to the GPU.\n",
        "            if using_GPU:\n",
        "                reshaped_images = reshaped_images.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "            # Run the forward pass through the model to get predicted log distribution.\n",
        "            # predicted shape: (batch_size, 10) (since there are 10 classes)\n",
        "            predicted = fashionmnist_ffnn_clf(reshaped_images)\n",
        "\n",
        "            # Calculate the loss\n",
        "            batch_loss = nll_criterion(predicted, labels)\n",
        "\n",
        "            # Clear the gradients as we prepare to backprop.\n",
        "            ffnn_optimizer.zero_grad()\n",
        "\n",
        "            # Backprop (backward pass), which calculates gradients.\n",
        "            batch_loss.backward()\n",
        "\n",
        "            # Take a gradient step to update parameters.\n",
        "            ffnn_optimizer.step()\n",
        "\n",
        "            # Increment gradient update counter.\n",
        "            num_iter += 1\n",
        "\n",
        "            # Calculate test set loss and accuracy every 500 gradient updates\n",
        "            # It's standard to have this as a separate evaluate function, but\n",
        "            # we'll place it inline for didactic purposes.\n",
        "            if num_iter % 500 == 0:\n",
        "                # Set model to eval mode, which turns off dropout.\n",
        "                fashionmnist_ffnn_clf.eval()\n",
        "                # Counters for the num of examples we get right / total num of examples.\n",
        "                num_correct = 0\n",
        "                total_examples = 0\n",
        "                total_test_loss = 0\n",
        "\n",
        "                # Iterate over the test dataloader\n",
        "                for (test_images, test_labels) in test_dataloader:\n",
        "                    # Reshape images from (batch_size, 1, 28, 28) to (batch_size, 784) again\n",
        "                    reshaped_test_images = test_images.view(-1, 784)\n",
        "\n",
        "                    # Wrap test data in Variable, like we did earlier.\n",
        "                    # We set volatile=True bc we don't need history; speeds up inference.\n",
        "                    reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
        "                    test_labels = Variable(test_labels, volatile=True)\n",
        "\n",
        "                    # If we're using the GPU, move tensors to the GPU.\n",
        "                    if using_GPU:\n",
        "                        reshaped_test_images = reshaped_test_images.cuda()\n",
        "                        test_labels = test_labels.cuda()\n",
        "\n",
        "                    # Run the forward pass to get predicted distribution.\n",
        "                    predicted = fashionmnist_ffnn_clf(reshaped_test_images)\n",
        "\n",
        "                    # Calculate loss for this test batch. This is averaged, so multiply\n",
        "                    # by the number of examples in batch to get a total.\n",
        "                    total_test_loss += nll_criterion(\n",
        "                        predicted, test_labels).data * test_labels.size(0)\n",
        "\n",
        "                    # Get predicted labels (argmax)\n",
        "                    # We need predicted.data since predicted is a Variable, and torch.max\n",
        "                    # expects a Tensor as input. .data extracts Tensor underlying Variable.\n",
        "                    _, predicted_labels = torch.max(predicted.data, 1)\n",
        "\n",
        "                    # Count the number of examples in this batch\n",
        "                    total_examples += test_labels.size(0)\n",
        "\n",
        "                    # Count the total number of correctly predicted labels.\n",
        "                    # predicted == labels generates a ByteTensor in indices where\n",
        "                    # predicted and labels match, so we can sum to get the num correct.\n",
        "                    num_correct += torch.sum(predicted_labels == test_labels.data)\n",
        "                accuracy = 100 * num_correct / total_examples\n",
        "                average_test_loss = total_test_loss / total_examples\n",
        "                print(\"Iteration {}. Test Loss {}. Test Accuracy {}.\".format(\n",
        "                    num_iter, average_test_loss, accuracy))\n",
        "                # Set the model back to train mode, which activates dropout again.\n",
        "                fashionmnist_ffnn_clf.train()\n",
        "\n",
        "                # Check for early stopping\n",
        "                if average_test_loss < best_test_loss:\n",
        "                    best_test_loss = average_test_loss\n",
        "                    epochs_without_improvement = 0\n",
        "                else:\n",
        "                    epochs_without_improvement += 1\n",
        "                    if epochs_without_improvement >= patience:\n",
        "                        print(\"Early stopping triggered. No improvement in test loss for {} epochs.\".format(patience))\n",
        "                        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGy3QG4UgDPD",
        "outputId": "b9ec50e5-01b8-4d8f-b28b-2d759eeccda4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FeedForwardNN_new(\n",
            "  (hidden_layers): ModuleList(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (output_projection): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (nonlinearity): ReLU()\n",
            ")\n",
            "Shapes of model parameters:\n",
            "[torch.Size([512, 784]), torch.Size([512]), torch.Size([256, 512]), torch.Size([256]), torch.Size([128, 256]), torch.Size([128]), torch.Size([10, 128]), torch.Size([10])]\n",
            "Starting epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-ac1ab897312a>:68: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  reshaped_test_images = Variable(reshaped_test_images, volatile=True)\n",
            "<ipython-input-15-ac1ab897312a>:69: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  test_labels = Variable(test_labels, volatile=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 500. Test Loss 0.6402992010116577. Test Accuracy 77.0199966430664.\n",
            "Starting epoch 2\n",
            "Iteration 1000. Test Loss 0.5309149622917175. Test Accuracy 80.86000061035156.\n",
            "Iteration 1500. Test Loss 0.489239364862442. Test Accuracy 82.76000213623047.\n",
            "Starting epoch 3\n",
            "Iteration 2000. Test Loss 0.456836462020874. Test Accuracy 83.66999816894531.\n",
            "Iteration 2500. Test Loss 0.43717920780181885. Test Accuracy 84.41999816894531.\n",
            "Starting epoch 4\n",
            "Iteration 3000. Test Loss 0.4248334467411041. Test Accuracy 85.06999969482422.\n",
            "Iteration 3500. Test Loss 0.4103059470653534. Test Accuracy 85.41000366210938.\n",
            "Starting epoch 5\n",
            "Iteration 4000. Test Loss 0.4021386504173279. Test Accuracy 85.5999984741211.\n",
            "Iteration 4500. Test Loss 0.387502521276474. Test Accuracy 86.08000183105469.\n",
            "Starting epoch 6\n",
            "Iteration 5000. Test Loss 0.38503211736679077. Test Accuracy 85.94999694824219.\n",
            "Iteration 5500. Test Loss 0.3722062408924103. Test Accuracy 86.58000183105469.\n",
            "Starting epoch 7\n",
            "Iteration 6000. Test Loss 0.3889525830745697. Test Accuracy 86.01000213623047.\n",
            "Iteration 6500. Test Loss 0.3635512590408325. Test Accuracy 86.9000015258789.\n",
            "Starting epoch 8\n",
            "Iteration 7000. Test Loss 0.3623625636100769. Test Accuracy 86.9000015258789.\n",
            "Iteration 7500. Test Loss 0.3575786352157593. Test Accuracy 87.31999969482422.\n",
            "Starting epoch 9\n",
            "Iteration 8000. Test Loss 0.3680683672428131. Test Accuracy 86.83999633789062.\n",
            "Starting epoch 10\n",
            "Iteration 8500. Test Loss 0.3448880612850189. Test Accuracy 87.58000183105469.\n",
            "Iteration 9000. Test Loss 0.3456488251686096. Test Accuracy 87.70999908447266.\n"
          ]
        }
      ],
      "source": [
        "# L2 regularization + Dropout on the best combination: ReLU Activation Function + Adam Optimizer\n",
        "l2_ffnn_clf_2 = FeedForwardNN_new(input_size=784, num_classes=10, hidden_dims = [512, 256, 128], dropout=0.2, activation_fn= nn.ReLU())\n",
        "if using_GPU:\n",
        "    l2_ffnn_clf_2 = l2_ffnn_clf_2.cuda()\n",
        "\n",
        "print(l2_ffnn_clf_2)\n",
        "\n",
        "parameters = l2_ffnn_clf_2.parameters()\n",
        "\n",
        "print(\"Shapes of model parameters:\")\n",
        "print([x.size() for x in list(parameters)])\n",
        "\n",
        "# Set up criterion for calculating loss\n",
        "nll_criterion = nn.NLLLoss()\n",
        "\n",
        "lr = 0.1\n",
        "momentum = 0.9\n",
        "# Set up an optimizer for updating the parameters of fashionmnist_ffnn_clf\n",
        "Adam_optimizer = optim.Adam(l2_ffnn_clf_2.parameters(), lr=0.0001, weight_decay=1e-5)\n",
        "\n",
        "ffnn_optimizer = Adam_optimizer\n",
        "early_stop_training_phase(ffnn_optimizer, l2_ffnn_clf_2, patience=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dttwPubyctxJ"
      },
      "source": [
        "**Report on early stopping:**\n",
        "\n",
        "**Results:**\n",
        "- Implementing early stopping resulted in a marginal improvement in test accuracy, increasing from 87.51% to 87.71%. However, it led to a slight increase in test loss from 0.3429 to 0.3456.\n",
        "- The model continued training without triggering early stopping because the test loss continued to improve or remained the same for at least patience epochs.\n",
        "\n",
        "**Discussion:**\n",
        "\n",
        "The introduction of early stopping in the training process did not significantly affect the results. The model continued training without triggering early stopping because the test loss either continued to improve or remained stable for a specified number of epochs. This indicates that the early stopping mechanism was appropriately designed to allow continued training when the model was still improving.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "The introduction of early stopping did not significantly alter the results, as the model continued training without interruption. This indicates that the training process was effective, with the test loss either improving or stabilizing over time. While early stopping may not have led to notable improvements in this instance, its presence as a safeguard against overfitting remains valuable. This highlights the importance of robust training procedures and effective implementation of techniques such as early stopping to ensure optimal model performance."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
